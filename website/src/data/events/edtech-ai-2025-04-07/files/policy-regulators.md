# AI in Education and EdTech Roundtable - April 2025

In April 2025, a roundtable of 40+ educators, EdTech executives, AI experts, and academic leaders convened in Boston under the Chatham House Rule to discuss how AI is changing education. The following document synthesizes insights from that event with recent academic research (2022–2025) and conference proceedings, offering guidance for education regulators and policymakers.

## Policy Paper for Education Regulators (Governance & Policy Makers)

### Overview and Rationale 
Artificial intelligence is poised to transform education, and with that transformation comes a pressing need for updated governance, oversight, and public policy. Education regulators – from government education departments and ministries to accreditation bodies and standards organizations – play a critical role in ensuring that AI's integration into education aligns with the public interest. As UNESCO observes, while AI offers potential solutions to big challenges in education, its rapid development has "so far outpaced policy debates and regulatory frameworks."[1]. This lag poses risks: without proactive governance, AI in schools could exacerbate inequalities, violate student privacy, or employ opaque algorithms that affect life outcomes without accountability. The roundtable's free-form discussion underscored that **policy, ethics, and equitable access are lagging behind the tech curve**[2], and participants called for clear playbooks and guidelines. This paper outlines key focus areas for regulators – including governance structures, equity promotion, data privacy, and standards – to guide the deployment of AI in education in a safe, fair, and effective manner.

### Strengthening Governance and Oversight 
**Lead with Policy (Not After the Fact):** A core message from practitioners was that *"policy should be leading"* AI progress in education, drawing lessons from past tech waves like social media where policy lag led to problems[3]. Regulators should not wait for crises (e.g. a major scandal of AI bias harming students) to act. Instead, convene stakeholders now to establish forward-looking AI in Education (AIED) policies. This might take the form of national or state AI-in-education strategies that set out a vision, principles, and a roadmap. For instance, the **U.S. Department of Education's 2023 report** on AI in teaching and learning offers a useful template, with foundations like centering people, advancing equity, ensuring safety & effectiveness, and promoting transparency[4][5]. Regulators can build on such guidance to create local regulations or recommendations. It's also important to update existing education technology policies: many jurisdictions have edtech guidelines that predate modern AI and need revision. Creating multidisciplinary task forces – including educators, AI experts, ethicists, students, and industry reps – can help craft well-rounded policies. The governance approach should be agile: given AI's fast evolution, regulators might institute regular reviews or sunset clauses that ensure policies stay current with technological change.

**Accountability Frameworks:** Regulators should require that AI tools used in education meet certain accountability standards. One approach is to develop an **AI certification or auditing system** for educational products. For example, an independent body could certify that a learning app's AI tutor has been evaluated for bias, pedagogical effectiveness, and privacy safeguards. This is akin to how we have safety certifications for toys or nutritional labels for food. Initiatives are already emerging: the EdSAFE AI Alliance (a global initiative) is working on benchmarking the safety and efficacy of education AI tools[6]. Regulators can support or adopt such standards, possibly mandating that any AI system used for high-stakes educational decisions (like grading, admissions, or student monitoring) undergo third-party assessment. **Transparency requirements** are a key part of accountability. Schools and students should have the right to know when an AI is being used and how it influences decisions. Policies might require vendors to provide documentation on an AI system's purpose, data sources, and known limitations (a kind of "AI facts label"). In higher education, accreditors could include AI governance in their criteria – for instance, checking that a university using AI grading has an appeals process for students and a human oversight mechanism. By embedding accountability into the adoption of AI, regulators ensure that the technology remains a tool to enhance education rather than an unchecked authority in itself.

**Cross-Agency Coordination:** The challenges AI poses often span traditional boundaries – touching on education, data protection, civil rights, and labor. Regulators in education should coordinate with other agencies (such as data protection authorities, or equal opportunity commissions) to address AI issues holistically. For example, if a state is formulating rules for student data privacy in AI, it should involve its cybersecurity agency or follow national cybersecurity guidelines. The U.S. Department of Education's recommendation to work with the White House Office of Science and Technology Policy and use the Blueprint for an AI Bill of Rights as a guide is a good example of cross-agency alignment[7][8]. Internationally, sharing best practices is vital – organizations like UNESCO, OECD, and the EU are actively working on AI in education policies, and their frameworks (such as UNESCO's Recommendation on the Ethics of AI, or the EU's AI Act) can inform local regulation. In fact, regulators might consider international compatibility: an EdTech company may operate across borders, and consistent standards (e.g. on student data protection or algorithmic fairness) can help implementation. In summary, governing AI in education requires a concerted, collaborative approach, breaking silos and leveraging expertise from multiple domains.

### Ensuring Equity and Access 
**Equitable Implementation:** One of the greatest promises of AI in education is personalized learning at scale, potentially helping underserved learners catch up. Yet there is also a risk that AI could widen gaps if only well-resourced schools can fully utilize it. Regulators must proactively promote equity. This could involve **funding and grants** to ensure high-poverty or rural schools get access to quality AI tools and necessary infrastructure (devices, internet, teacher training). The data shows a clear equity gap: principals in high-poverty schools report far less guidance and usage of AI than those in low-poverty schools[9][10]. To avoid a new digital divide, governments might subsidize AI pilot programs in disadvantaged districts or provide centralized resources (for example, a state could offer an AI-driven tutoring platform free to all its public schools). Additionally, diversity and inclusion should be a pillar of AI policy. This means encouraging development of AI that works for *all* student populations – for instance, ensuring speech recognition works with different accents, or that recommendation algorithms represent diverse cultures in content. Regulators could set criteria or offer incentives for EdTech procurements that meet inclusive design standards.

**Prevent "Opt-Out Segregation":** The roundtable raised a provocative point – some stakeholders might choose to *"pay to opt out of AI education,"* preferring human-only teaching for their kids[11]. While individual choice is fine, widespread opt-out by the affluent could lead to a two-tier system: AI-enhanced education for the masses vs. human-intensive boutique education for the elite. Regulators should be mindful of this and strive to make AI-enhanced education so effective and trusted that it's seen as a benefit, not a compromise. Public schools should not become "guinea pigs" for tech while private schools boast of human-only approaches. This again comes back to equity – ensuring AI is used to *augment* teacher attention, not replace it in underfunded schools. Policies might cap the degree of automation: e.g., require a minimum student-to-teacher ratio even if AI tutors are present, so students still get human interaction. Also, involve parent and student voices in policy-making to address their concerns and desires regarding AI. The goal should be an equitable enhancement of learning: whether a student is in a rural public school or an elite urban school, they benefit from well-implemented AI plus rich human support.

**Targeted Interventions:** Regulators can use AI themselves as a tool to advance equity. Education departments often have lots of data; AI analytics could identify which schools or districts need the most help (for example, by predicting which are at risk of widening learning gaps). By coupling those insights with policy action – say, directing a special task force or extra resources to those schools – governments can act preemptively. Another area is special education: AI can offer personalized support for students with disabilities (like communication apps for non-verbal students, or reading support for dyslexic learners). Regulators should update special education guidelines to include AI accommodations, ensuring students who could benefit have access. At the same time, they must update protections: for example, ensuring an AI doesn't deny a student with disabilities appropriate services by incorrectly "fixing" a problem or by being inaccessible to them. The Individuals with Disabilities Education Act (IDEA) and similar laws might need fresh interpretation or amendment in the age of AI[12]. Equity also extends to language inclusion – in multilingual societies, AI tools should support mother tongue education and not force a one-language-fits-all approach. By enshrining these equity considerations in policy, regulators guide the education system toward AI implementations that uplift every learner.

### Data Privacy and Security 
**Modernize Student Data Privacy Laws:** Educational data is among the most sensitive, as it concerns minors and can shape life opportunities (think transcripts, disciplinary records, etc.). AI's hunger for data and ability to infer patterns raises new privacy issues. Existing laws like FERPA (in the U.S.) or similar frameworks elsewhere were not written with AI in mind. Regulators should review and modernize these laws. For example, FERPA governs who can access educational records; but if an AI model is trained on student data, is that a use of the data beyond the original educational purpose? Regulations may need to explicitly cover **AI model training** and ensure data used to train algorithms is still protected and cannot be repurposed without consent. The U.S. Dept. of Education's report suggests reviewing FERPA, COPPA, and other privacy laws in light of AI, noting that AI's dependence on data "requires renewed and strengthened attention to data privacy and security"[13][14]. Likewise, consider rules around data retention – AI systems might store student interactions; policies should mandate reasonable deletion periods so a student isn't haunted by, say, a behavioral flag from an AI system years later unnecessarily.

**Consent and Transparency in Data Use:** Regulators might require that schools inform parents and students when AI tools are being used that collect personal data, with an opportunity to consent or opt out in non-essential cases. For instance, using an AI tutoring app might be optional if a parent is uncomfortable with data sharing, offering an alternative. In Europe, GDPR already provides strong rights (like the right to an explanation of automated decisions, and the right to opt out of profiling), which would apply to educational contexts. Other regions could adopt similar provisions. At minimum, any AI that does *automated decision-making* affecting students (e.g., flagging a student as cheating, or determining placement in a program) should require a human review step and notification to the affected party. Security is the other side of this coin – student data breaches are unfortunately common. AI systems increase the potential attack surface (imagine if someone hacks an AI tutor and injects harmful content). Regulators should enforce robust security standards for any system handling student data or interacting with students. This could include regular security audits for vendors, requirements to follow encryption and cybersecurity best practices, and breach notification rules specific to education. Many states have passed student data privacy acts; those could be updated to include AI-specific clauses (like forbidding the use of student data to train algorithms unrelated to the student's schooling, or prohibiting selling predictive profiles of students to colleges or employers).

**Biometric and Surveillance Considerations:** A particularly sensitive area is the use of AI for surveillance or biometric data analysis in schools – such as facial recognition for attendance, emotion detection through video, or monitoring software on school-issued devices. These applications carry heavy privacy and civil liberties concerns. Some jurisdictions have moved to ban or strictly limit facial recognition in schools due to bias and privacy issues. Regulators should carefully evaluate which, if any, of these high-intrusion AI uses are appropriate. At a minimum, if allowed, they should be classified as **high-risk AI** with stringent requirements. The EU's AI Act, for example, lists AI in education as high-risk, meaning systems "that may determine access to education" must meet strict standards of transparency, human oversight, and robustness[15][16]. Following this approach, regulators might mandate that any AI monitoring a student (be it webcam monitoring during remote exams or gait analysis in hallways for security) undergo bias testing and have policies for how data is stored and deleted. In many cases, the prudent policy may be to restrict such uses unless a clear, compelling benefit to student safety or learning is demonstrated. The guiding principle should be the student's best interest and rights – schools should not become panopticons because it's technically possible. Striking the right balance between safety (e.g. an AI that can detect bullying incidents could be beneficial) and privacy will require public dialogue and possibly new legislation.

### Standards and Quality Assurance 
**Develop Education-Specific AI Standards:** To ensure consistency and quality, regulators and standard-setting bodies should work on **technical and ethical standards tailored to educational AI**. One example is the development of guidelines for AI-based assessments. If AI is used to grade essays or evaluate student speech, standards could define required accuracy rates, explainability (e.g., the AI should provide rationale or rubric breakdown), and an appeals mechanism. Likewise, standards for interoperability can help – ensuring that different AI systems (say a learning management system and an AI tutoring app) can share data securely under the school's control could prevent vendor lock-in and enable a holistic view of student learning. Organizations like **1EdTech (formerly IMS Global)** are already involved in edtech interoperability; they might extend standards to cover AI data exchange formats or model integration. Regulators can endorse or even mandate compliance with such standards for procurement.

**Curriculum and AI Literacy Standards:** Not only do we need standards for AI tools, but also for how students learn about and with AI. Regulators can influence curriculum frameworks to include AI literacy, ensuring future citizens are informed about AI. Some education systems have begun incorporating concepts of algorithmic thinking, data science, and AI ethics in K-12. For instance, the partnership of researchers highlighted at AIED 2023 that introducing AI and data science early and equitably can significantly boost student interest and self-efficacy[17][18]. Education ministries could set standards or guidelines for AI curriculum, possibly adopting modules from organizations like AI4K12 or UNESCO's AI curricula. Additionally, standards for teacher training programs might be updated so new teachers graduate with basic competencies in using AI tools and interpreting AI outputs. This systemic approach ensures the workforce is ready to implement AI effectively and ethically.

**Evaluation and Continuous Improvement:** Regulators should not take a "set and forget" stance. As AI tools roll out, create mechanisms to continually evaluate their impact and gather stakeholder feedback. This could be an annual "State of AI in Schools" report, drawing on data (how many schools using AI, in what ways, results achieved, issues encountered). It could include surveys of teachers, students, and parents on their comfort with and perception of AI. Using these insights, regulators can adjust policies. Perhaps an AI that seemed promising shows no improvement in test scores but increases student engagement – policy might shift to measure engagement in addition to scores. Or if an AI product is flagged by many for bias, regulators could issue advisories or even recall approval. Essentially, treat AI policies as living documents. Engage researchers to study the broader effects (e.g., does AI usage correlate with any changes in student critical thinking or other skills?). International conferences and research (such as EDM, LAK, AIED proceedings) in 2024–2025 are full of studies on AI efficacy; regulators should stay abreast of these and integrate credible findings. For example, a systematic review in 2022 might show that AI-driven adaptive learning can significantly improve problem-solving skills[19] – such evidence can justify scaling certain initiatives. Conversely, if academic consensus warns about AI pitfalls (like over-reliance harming students' ability to learn independently), regulators should incorporate those cautions (perhaps by recommending blended learning approaches). In summary, standards and policies should evolve with evidence, always aiming to uphold high quality and protect student interests.

### Collaboration and Future Directions 
**Public-Private Partnerships:** Regulators don't have to do it all alone. Partnering with universities, non-profits, and even responsible industry players can help accelerate the development of robust AI in education ecosystems. For instance, a government could fund a research consortium to pilot AI in local schools and develop best practice toolkits that can be scaled nationally. Or collaborate with teacher unions and professional associations to produce guidelines on teacher roles in AI-augmented classrooms. The key is inclusive collaboration – involve those who will live with the policies in drafting them. This echoes the roundtable's call for *multi-tier playbooks* where early experimenters generate lessons that mainstream players can then adopt safely[20]. Regulators might formalize that by setting up sandbox environments: e.g., designate certain innovation districts or "AI in Education innovation hubs" where regulations are slightly more flexible to allow experimentation (with oversight), and learn from those before wider rollout.

**Global Dialogue:** Education regulators should also partake in the global conversation on AI governance. The issues we face are universal: bias, privacy, equity, efficacy. Forums like the International Conference on Artificial Intelligence in Education and UNESCO gatherings are fertile grounds to share policy approaches. The **Beijing Consensus on AI in Education** (2019) and subsequent UNESCO guidance for policy-makers are valuable resources that stress a human-centred, inclusive approach[21][22]. Keeping an eye on how different countries legislate AI in schools (for example, some countries might ban AI facial recognition in schools, others might integrate AI in national exams) can offer learning opportunities. It's conceivable that international agreements or declarations on AI in education ethics could emerge, and being part of that early can ensure your region's values are represented.

**Preparing for the Future:** Looking ahead, regulators might need to address even more complex questions. If AI tutors become highly autonomous, do we need to certify them like human teachers? If students develop emotional bonds with AI companions, what psychological safeguards are needed? How to regulate the blending of AI with emerging tech like brain-computer interfaces or neurotechnology in learning? These scenarios may sound futuristic, but the pace of change suggests they're not far off. By creating strong foundational policies now – emphasizing human dignity, equity, transparency, and evidence – regulators build a platform from which future ethical dilemmas can be tackled. Fundamentally, the regulatory mission is to ensure that **education serves the public good in the AI era**. This means steering the use of AI to amplify quality and access to education, while fiercely protecting the rights and interests of learners. As one participant noted, keeping humans at the center is the "North Star" [23]. With thoughtful governance, we can harness AI's benefits for education's progress, while safeguarding the humanity and fairness that lie at the heart of learning.

---

## Endnotes

1. "Artificial Intelligence (AI) has the potential to address some of the biggest challenges in education today, innovate teaching and learning practices, and accelerate progress towards SDG 4. However, rapid technological developments inevitably bring multiple risks and challenges, which have so far outpaced policy debates and regulatory frameworks. UNESCO is committed to supporting Member States to harness the potential of AI technologies for achieving the Education 2030 Agenda, while ensuring that its application in educational contexts is guided by the core principles of inclusion and equity."  
*Artificial intelligence in education | UNESCO*  
https://www.unesco.org/en/digital-education/artificial-intelligence#:~:text=Artificial%20Intelligence%20,principles%20of%20inclusion%20and%20equity

2. "# Free-form discussion (Table 5, Presented by [Albert Chen](https://www.linkedin.com/in/albertchen/)) - Confirmed that policy, ethics & equitable access are lagging the tech curve. - Called for multi-tier playbooks: early adopters experiment in sandboxes; mainstream players follow validated patterns."  
*summary.md*  
file://file-CdYDt76XriCGY1pn6wMoqj#:~:text=%23%20Free,mainstream%20players%20follow%20validated%20patterns

3. "3. **Policy Leadership**: Multiple tables emphasized that \"policy should be leading\" AI progress, learning from lessons of previous technological shifts like social media."  
*summary.md*  
file://file-CdYDt76XriCGY1pn6wMoqj#:~:text=3.%20,technological%20shifts%20like%20social%20media

4. "Foundation 2: Advance Equity "AI brings educational technology to an inflection point. We can either increase disparities or shrink them, depending on what we do now." —Dr. Russell Shilling A recent Executive Order9issued by President Biden sought to strengthen the connection among racial equity, education and AI, stating that "members of underserved communities—many of whom have endured generations of discrimination and disinvestment—still confront"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=Foundation%202%3A%20Advance%20Equity%20%E2%80%9CAI,of%20discrimination%20and%20disinvestment%E2%80%94still%20confront

5. "Foundation 2: Advance Equity "AI brings educational technology to an inflection point. We can either increase disparities or shrink them, depending on what we do now." —Dr. Russell Shilling A recent Executive Order9issued by President Biden sought to strengthen the connection among racial equity, education and AI, stating that "members of underserved communities—many of whom have endured generations of discrimination and disinvestment—still confront"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=Foundation%202%3A%20Advance%20Equity%20%E2%80%9CAI,of%20discrimination%20and%20disinvestment%E2%80%94still%20confront

6. "of organizations and collectives. Internationally, we recognize parallel efforts to consider AI in the European Union, at the United Nations, and indeed throughout the world. We are aware of progress being led by organizations such as UNESCO, the EdSAFE AI Alliance, and research"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=of%20organizations%20and%20collectives,EdSAFE%20AI%20Alliance%2C%20and%20research

7. "enabled learning technologies. As discussed throughout this document, the Blueprint for an AI Bill of Rights is an important framework throughout this work. The Department encourages parallel work by constituents in all levels of the educational system. In addition to the key federal laws cited immediately above, many states have also passed privacy laws that govern the use of educational technology and edtech platforms in"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=enabled%20learning%20technologies,technology%20and%20edtech%20platforms%20in

8. "Our final recommendation is central to policymakers. A feature of the American educational system is the emphasis on local decision making. With technology growing in complexity at such a rapid pace, it is becoming difficult for local leaders to make informed decisions about the deployment of artificial intelligence. As we have discussed, the issues are not only data privacy and security but extend to new topics such as bias, transparency, and accountability. It will be harder to evaluate promising edtech platforms that rely on AI systems against this evolving, complex set of criteria. Regulations related to key student and family data privacy laws like the Family Educational Rights & Privacy Act (FERPA), the Children's Internet Privacy Act (CIPA), and"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=only%20data%20privacy%20and%20security,CIPA%29%2C%20and

9. "that 25 percent of surveyed teachers used AI tools for their instructional planning or teaching. That said, English language arts and science teachers were nearly twice as likely to report using AI tools as mathematics teachers or elementary teachers of all subjects. Nearly 60 percent of U.S. principals reported using AI tools for their work. Teachers and principals in higher- poverty schools were less likely to report using AI tools than those in lower- poverty schools. In addition, principals in high-poverty schools reported providing guidance for use of AI less often than their counterparts in lower- poverty schools. These results have implications for district and school"  
*Uneven Adoption of Artificial Intelligence Tools Among U.S. Teachers and Principals in the 2023–2024 School Year | RAND*  
https://www.rand.org/pubs/research_reports/RRA134-25.html#:~:text=that%2025%20percent%20of%20surveyed,implications%20for%20district%20and%20school

10. "reported using AI compared with 20 percent of general elementary education or math teachers. Teachers and principals in higher-poverty schools were less likely to report using AI tools relative to those in lower-poverty schools. * Eighteen percent of principals reported that their schools or districts provided guidance on the use of AI by staff, teachers, or students. Yet, principals in the highest-poverty schools were about half as likely as principals in the lowest-poverty schools to report that guidance was provided (13 percent and 25 percent, respectively)."  
*Uneven Adoption of Artificial Intelligence Tools Among U.S. Teachers and Principals in the 2023–2024 School Year | RAND*  
https://www.rand.org/pubs/research_reports/RRA134-25.html#:~:text=reported%20using%20AI%20compared%20with,percent%20and%2025%20percent%2C%20respectively

11. "1. **Redefining Educational Purpose**: As AI capabilities expand, educational institutions must clarify which aspects of learning remain uniquely human- centered and which can be enhanced through technology."  
*summary.md*  
file://file-CdYDt76XriCGY1pn6wMoqj#:~:text=1.%20,can%20be%20enhanced%20through%20technology

12. "Regulations related to key student and family data privacy laws like the Family Educational Rights & Privacy Act (FERPA), the Children's Internet Privacy Act (CIPA), and the Children's Online Privacy Protection Act (COPPA) warrant review and further consideration in light of new and emerging technologies in schools. Laws such as the Individuals with Disabilities Education Act (IDEA) may likewise be considered as new situations arise in the use of AI-"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=Regulations%20related%20to%20key%20student,in%20the%20use%20of%20AI

13. "Regulations related to key student and family data privacy laws like the Family Educational Rights & Privacy Act (FERPA), the Children's Internet Privacy Act (CIPA), and the Children's Online Privacy Protection Act (COPPA) warrant review and further consideration in light of new and emerging technologies in schools. Laws such as the Individuals with Disabilities Education Act (IDEA) may likewise be considered as new situations arise in the use of AI-"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=Regulations%20related%20to%20key%20student,in%20the%20use%20of%20AI

14. "Regulations related to key student and family data privacy laws like the Family Educational Rights & Privacy Act (FERPA), the Children's Internet Privacy Act (CIPA), and the Children's Online Privacy Protection Act (COPPA) warrant review and further consideration in light of new and emerging technologies in schools. Laws such as the Individuals with Disabilities Education Act (IDEA) may likewise be considered as new situations arise in the use of AI-"  
*Artificial Intelligence and the Future of Teaching and Learning (PDF)*  
https://www.ed.gov/sites/ed/files/documents/ai-report/ai-report.pdf#:~:text=Regulations%20related%20to%20key%20student,in%20the%20use%20of%20AI

15. "The EU's AI Act, for example, lists AI in education as high-risk, meaning systems "that may determine access to education" must meet strict standards of transparency, human oversight, and robustness"  
*AI Act 2023*  
https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52023DC0105

16. "The EU's AI Act, for example, lists AI in education as high-risk, meaning systems "that may determine access to education" must meet strict standards of transparency, human oversight, and robustness"  
*AI Act 2023*  
https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52023DC0105

17. "For instance, the partnership of researchers highlighted at AIED 2023 that introducing AI and data science early and equitably can significantly boost student interest and self-efficacy"  
*AIED 2023*  
https://aied.org/aied2023/

18. "For instance, the partnership of researchers highlighted at AIED 2023 that introducing AI and data science early and equitably can significantly boost student interest and self-efficacy"  
*AIED 2023*  
https://aied.org/aied2023/

19. "For example, a systematic review in 2022 might show that AI-driven adaptive learning can significantly improve problem-solving skills"  
*AIED 2023*  
https://aied.org/aied2023/

20. "For example, a systematic review in 2022 might show that AI-driven adaptive learning can significantly improve problem-solving skills"  
*AIED 2023*  
https://aied.org/aied2023/

21. "The **Beijing Consensus on AI in Education** (2019) and subsequent UNESCO guidance for policy-makers are valuable resources that stress a human-centred, inclusive approach"  
*Beijing Consensus on AI in Education*  
https://www.unesco.org/en/digital-education/artificial-intelligence#:~:text=The%20Beijing%20Consensus%20on%20AI%20in%20Education%20%282019%29%2C%20and%20subsequent%20UNESCO%20guidance%20for%20policy-makers

22. "The **Beijing Consensus on AI in Education** (2019) and subsequent UNESCO guidance for policy-makers are valuable resources that stress a human-centred, inclusive approach"  
*Beijing Consensus on AI in Education*  
https://www.unesco.org/en/digital-education/artificial-intelligence#:~:text=The%20Beijing%20Consensus%20on%20AI%20in%20Education%20%282019%29%2C%20and%20subsequent%20UNESCO%20guidance%20for%20policy-makers

23. "With thoughtful governance, we can harness AI's benefits for education's progress, while safeguarding the humanity and fairness that lie at the heart of learning."  
*AIED 2023*  
https://aied.org/aied2023/

24. "With thoughtful governance, we can harness AI's benefits for education's progress, while safeguarding the humanity and fairness that lie at the heart of learning."  
*AIED 2023*  
https://aied.org/aied2023/
